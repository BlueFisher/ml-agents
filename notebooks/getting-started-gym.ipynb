{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML-Agents Toolkit\n",
    "##  Gym Wrapper Basics\n",
    "This notebook contains a walkthrough of the basic functions of the Python Gym Wrapper for the Unity ML-Agents toolkit. For instructions on building a Unity environment, see [here](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started-with-Balance-Ball.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-Agent Environments\n",
    "\n",
    "The first five steps show how to use the `UnityEnv` wrapper with single-agent environments. See below step five for how to use with multi-agent environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load dependencies\n",
    "\n",
    "The following loads the necessary dependencies and checks the Python version (at runtime). ML-Agents Toolkit (v0.3 onwards) requires Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:\n",
      "3.6.5 (default, Jun 17 2018, 12:13:06) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.2)]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from gym_unity.envs import UnityEnv\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Python version:\")\n",
    "print(sys.version)\n",
    "\n",
    "# check Python version\n",
    "if (sys.version_info[0] < 3):\n",
    "    raise Exception(\"ERROR: ML-Agents Toolkit (v0.3 onwards) requires Python 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Start the environment\n",
    "`UnityEnv` launches and begins communication with the environment when instantiated. We will be using the `GridWorld` environment. You will need to create an `envs` directory within the  `/python` subfolder of the repository, and build the GridWorld environment to that directory. For more information on building Unity environments, see [here](../docs/Learning-Environment-Executable.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:Connected new brain:\n",
      "GridWorld\n",
      "INFO:gym_unity:1 agents within environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UnityEnv instance>\n"
     ]
    }
   ],
   "source": [
    "env_name = \"../envs/GridWorld\"  # Name of the Unity environment binary to launch\n",
    "env = UnityEnv(env_name, worker_id=0, use_visual=True)\n",
    "\n",
    "# Examine environment parameters\n",
    "print(str(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Examine the observation and state spaces\n",
    "We can reset the environment to be provided with an initial observation of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent observations look like:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reset the environment\n",
    "initial_observation = env.reset()\n",
    "\n",
    "if len(env.observation_space.shape) == 1:\n",
    "    # Examine the initial vector observation\n",
    "    print(\"Agent state looks like: \\n{}\".format(initial_observation))\n",
    "else:\n",
    "    # Examine the initial visual observation\n",
    "    print(\"Agent observations look like:\")\n",
    "    if env.observation_space.shape[2] == 3:\n",
    "        plt.imshow(initial_observation[:,:,:])\n",
    "    else:\n",
    "        plt.imshow(initial_observation[:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Take random actions in the environment\n",
    "Once we restart an environment, we can step the environment forward and provide actions to all of the agents within the environment. Here we simply choose random actions using the `env.action_space.sample()` function.\n",
    "\n",
    "Once this cell is executed, 10 messages will be printed that detail how much reward will be accumulated for the next 10 episodes. The Unity environment will then pause, waiting for further signals telling it what to do next. Thus, not seeing any animation is expected when running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward this episode: -0.1099999975413084\n",
      "Total reward this episode: -0.1099999975413084\n",
      "Total reward this episode: 0.9000000022351742\n",
      "Total reward this episode: -1.0999999977648258\n",
      "Total reward this episode: -0.1099999975413084\n",
      "Total reward this episode: -1.0199999995529652\n",
      "Total reward this episode: -0.1099999975413084\n",
      "Total reward this episode: -0.1099999975413084\n",
      "Total reward this episode: -0.1099999975413084\n",
      "Total reward this episode: -0.1099999975413084\n"
     ]
    }
   ],
   "source": [
    "for episode in range(10):\n",
    "    initial_observation = env.reset()\n",
    "    done = False\n",
    "    episode_rewards = 0\n",
    "    while not done:\n",
    "        observation, reward, done, info = env.step(env.action_space.sample())\n",
    "        episode_rewards += reward\n",
    "    print(\"Total reward this episode: {}\".format(episode_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Close the environment when finished\n",
    "When we are finished using an environment, we can close it with the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:Environment shut down with return code 0.\n"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Agent Environments\n",
    "\n",
    "It is also possible to use the gym wrapper with multi-agent environments. For these environments, observations, rewards, and done flags will be provided in a list. Likewise, the environment will expect a list of actions when calling `step(action)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Start the environment\n",
    "\n",
    "We will use the `3DBall` environment for this walkthrough. For more information on building Unity environments, see [here](../docs/Learning-Environment-Executable.md). We will launch it from the `python/envs` sub-directory of the repo. Please create an `envs` folder if one does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:Connected new brain:\n",
      "3DBall\n",
      "INFO:gym_unity:12 agents within environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UnityEnv instance>\n"
     ]
    }
   ],
   "source": [
    "# Name of the Unity environment binary to launch\n",
    "multi_env_name = \"../envs/3DBall\"  \n",
    "multi_env = UnityEnv(multi_env_name, worker_id=1, \n",
    "                     use_visual=False, multiagent=True)\n",
    "\n",
    "# Examine environment parameters\n",
    "print(str(multi_env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the observation space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent observations look like: \n",
      "[-0.04583858 -0.02706882 -1.4387455   3.9411402   0.979342    0.\n",
      " -0.981       0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment\n",
    "initial_observations = multi_env.reset()\n",
    "\n",
    "if len(multi_env.observation_space.shape) == 1:\n",
    "    # Examine the initial vector observation\n",
    "    print(\"Agent observations look like: \\n{}\".format(initial_observations[0]))\n",
    "else:\n",
    "    # Examine the initial visual observation\n",
    "    print(\"Agent observations look like:\")\n",
    "    if multi_env.observation_space.shape[2] == 3:\n",
    "        plt.imshow(initial_observations[0][:,:,:])\n",
    "    else:\n",
    "        plt.imshow(initial_observations[0][:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take random steps in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward this episode: 1.59166672360152\n",
      "Total reward this episode: 1.2333333566784859\n",
      "Total reward this episode: 1.433333346620202\n",
      "Total reward this episode: 1.3833333905786276\n",
      "Total reward this episode: 1.0083333496004343\n",
      "Total reward this episode: 1.1750000240281224\n",
      "Total reward this episode: 1.2916667247191072\n",
      "Total reward this episode: 1.0250000301748514\n",
      "Total reward this episode: 1.2250000331550837\n",
      "Total reward this episode: 0.9750000406056643\n"
     ]
    }
   ],
   "source": [
    "for episode in range(10):\n",
    "    initial_observation = multi_env.reset()\n",
    "    done = False\n",
    "    episode_rewards = 0\n",
    "    while not done:\n",
    "        actions = [multi_env.action_space.sample() for agent in range(multi_env.number_agents)]\n",
    "        observations, rewards, dones, info = multi_env.step(actions)\n",
    "        episode_rewards += np.mean(rewards)\n",
    "        done = dones[0]\n",
    "    print(\"Total reward this episode: {}\".format(episode_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Close the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:Environment shut down with return code 0.\n"
     ]
    }
   ],
   "source": [
    "multi_env.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
